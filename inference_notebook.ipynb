{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e3090fc-da18-4a47-ba96-3e967557d165",
   "metadata": {},
   "source": [
    "# Inference Notebook\n",
    "\n",
    "Use this notebook to run inference using a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2fc9a3-6ebb-487e-a7ce-b9724fec373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.24.1)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dfdd6a7-d23a-4cb2-94b8-d8c90d87bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.19.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (3.11.0)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.6)\n",
      "Collecting pydantic<3,>=2.6 (from wandb)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.22.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.4.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions<5,>=4.4 (from wandb)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.19.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m143.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m298.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.22.0-py2.py3-none-any.whl (325 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.8/325.8 kB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: typing-extensions, smmap, setproctitle, sentry-sdk, protobuf, docker-pycreds, click, annotated-types, pydantic-core, gitdb, pydantic, gitpython, wandb\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "Successfully installed annotated-types-0.7.0 click-8.1.8 docker-pycreds-0.4.0 gitdb-4.0.12 gitpython-3.1.44 protobuf-5.29.3 pydantic-2.10.6 pydantic-core-2.27.2 sentry-sdk-2.22.0 setproctitle-1.3.4 smmap-5.0.2 typing-extensions-4.12.2 wandb-0.19.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045cb44f-68eb-43f7-b3c2-dc2e2cc22f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9755d90-5b33-4d3a-97ae-bd89f2ede704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.0+cu118)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.11.0.86)\n",
      "Collecting tqdm (from -r requirements.txt (line 3))\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m969.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting albumentations (from -r requirements.txt (line 4))\n",
      "  Downloading albumentations-2.0.4-py3-none-any.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib (from -r requirements.txt (line 5))\n",
      "  Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.24.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.16.0+cu118)\n",
      "Collecting transformers (from -r requirements.txt (line 8))\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py (from -r requirements.txt (line 9))\n",
      "  Downloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting segmentation_models_pytorch (from -r requirements.txt (line 10))\n",
      "  Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting warmup_scheduler (from -r requirements.txt (line 11))\n",
      "  Downloading warmup_scheduler-0.3.tar.gz (2.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting einops==0.6.1 (from -r requirements.txt (line 12))\n",
      "  Downloading einops-0.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting monai==1.1.0 (from -r requirements.txt (line 13))\n",
      "  Downloading monai-1.1.0-202212191849-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting efficientnet_pytorch (from -r requirements.txt (line 14))\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scikit-learn (from -r requirements.txt (line 15))\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting pandas (from -r requirements.txt (line 16))\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (9.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.1.0)\n",
      "Collecting numpy<2 (from -r requirements.txt (line 6))\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.10.0 (from albumentations->-r requirements.txt (line 4))\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations->-r requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from albumentations->-r requirements.txt (line 4)) (2.10.6)\n",
      "Collecting albucore==0.0.23 (from albumentations->-r requirements.txt (line 4))\n",
      "  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting opencv-python-headless>=4.9.0.80 (from albumentations->-r requirements.txt (line 4))\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting stringzilla>=3.10.4 (from albucore==0.0.23->albumentations->-r requirements.txt (line 4))\n",
      "  Downloading stringzilla-3.11.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.1/80.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations->-r requirements.txt (line 4))\n",
      "  Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 5))\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 5))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 5))\n",
      "  Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 5))\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->-r requirements.txt (line 5)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 7)) (2.31.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers->-r requirements.txt (line 8))\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 8))\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21 (from transformers->-r requirements.txt (line 8))\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers->-r requirements.txt (line 8))\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting pretrainedmodels>=0.7.1 (from segmentation_models_pytorch->-r requirements.txt (line 10))\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from segmentation_models_pytorch->-r requirements.txt (line 10)) (1.16.0)\n",
      "Collecting timm>=0.9 (from segmentation_models_pytorch->-r requirements.txt (line 10))\n",
      "  Downloading timm-1.0.14-py3-none-any.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 15))\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 15))\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->-r requirements.txt (line 16))\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->-r requirements.txt (line 16))\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting fsspec (from torch->-r requirements.txt (line 1))\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting munch (from pretrainedmodels>=0.7.1->segmentation_models_pytorch->-r requirements.txt (line 10))\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 4)) (2.27.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 7)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 7)) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 7)) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading monai-1.1.0-202212191849-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading albumentations-2.0.4-py3-none-any.whl (289 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading albucore-0.0.23-py3-none-any.whl (14 kB)\n",
      "Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading segmentation_models_pytorch-0.4.0-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.1/464.1 kB\u001b[0m \u001b[31m136.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m260.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 kB\u001b[0m \u001b[31m163.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m197.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.0/462.0 kB\u001b[0m \u001b[31m149.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading timm-1.0.14-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m178.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m232.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 kB\u001b[0m \u001b[31m141.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (632 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m191.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading stringzilla-3.11.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (304 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
      "Building wheels for collected packages: warmup_scheduler, efficientnet_pytorch, pretrainedmodels\n",
      "  Building wheel for warmup_scheduler (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for warmup_scheduler: filename=warmup_scheduler-0.3-py3-none-any.whl size=2968 sha256=245d310ae23baad883e7d10a1cbee8896163bafbacfc3746b7397b898cfb10b0\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/01/9e/d1820991c32916e9808c940f572b462f3e46427f3e76c4d852\n",
      "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=1884e873b2356643bc9d7d085748c2068fc85122e400d7381e7d7f83e7c7e96e\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=80434ef0a059ab5e5767784097472946f712bc78908b89d2d179ed3b526e6c0b\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
      "Successfully built warmup_scheduler efficientnet_pytorch pretrainedmodels\n",
      "Installing collected packages: warmup_scheduler, stringzilla, simsimd, pytz, tzdata, tqdm, threadpoolctl, safetensors, regex, numpy, munch, kiwisolver, joblib, fsspec, fonttools, einops, cycler, scipy, pandas, opencv-python-headless, huggingface-hub, h5py, contourpy, tokenizers, scikit-learn, monai, matplotlib, efficientnet_pytorch, albucore, transformers, timm, pretrainedmodels, albumentations, segmentation_models_pytorch\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed albucore-0.0.23 albumentations-2.0.4 contourpy-1.3.1 cycler-0.12.1 efficientnet_pytorch-0.7.1 einops-0.6.1 fonttools-4.56.0 fsspec-2025.2.0 h5py-3.12.1 huggingface-hub-0.28.1 joblib-1.4.2 kiwisolver-1.4.8 matplotlib-3.10.0 monai-1.1.0 munch-4.0.0 numpy-1.26.4 opencv-python-headless-4.11.0.86 pandas-2.2.3 pretrainedmodels-0.7.4 pytz-2025.1 regex-2024.11.6 safetensors-0.5.2 scikit-learn-1.6.1 scipy-1.15.2 segmentation_models_pytorch-0.4.0 simsimd-6.2.1 stringzilla-3.11.3 threadpoolctl-3.5.0 timm-1.0.14 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.49.0 tzdata-2025.1 warmup_scheduler-0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5041d1d0",
   "metadata": {
    "papermill": {
     "duration": 4.628791,
     "end_time": "2023-06-14T16:54:36.924059",
     "exception": false,
     "start_time": "2023-06-14T16:54:32.295268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "import cv2\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "\n",
    "import datetime\n",
    "# import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "099916b5",
   "metadata": {
    "papermill": {
     "duration": 67.942023,
     "end_time": "2023-06-14T16:55:44.878300",
     "exception": false,
     "start_time": "2023-06-14T16:54:36.936277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install /kaggle/input/einops/einops-0.6.1-py3-none-any.whl\n",
    "# !pip install /kaggle/input/monai-packages/monai-1.1.0-202212191849-py3-none-any.whl[\"einops\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7de1c-cc29-4108-aa2e-392c83bc33a5",
   "metadata": {},
   "source": [
    "To get the folders necessary for the path appends below, check the README for instructions.\n",
    "\n",
    "See the part that says  \n",
    "`!cd /root && git clone git@github.com:wolny/pytorch-3dunet.git && cd pytorch-3dunet && pip install -e .` You just need to grab the `get_model` function from this repo and grab the model directly.\n",
    "\n",
    "In this inference notebook we originally manually added the paths to `segmentation-models-pytorch`, `timm-pytorch-image-models`, `efficientnet-pytorch`, and `pretrainedmodels`, as well as the 3d-unet. On the actual notebook we submitted, we put the models on the github links above into the kaggle datasets below:\n",
    "\n",
    "https://www.kaggle.com/datasets/ryches/einops  \n",
    "https://www.kaggle.com/datasets/ryches/3d-unet  \n",
    "https://www.kaggle.com/datasets/ryches/unet3d  \n",
    "\n",
    "Here are the links to the other repos. You likely won't have to install these manually as they're included in `requirements.txt`. We're including links anyway in case you have to go that route. Make sure to edit the `sys.path.append` calls below to include if you're grabbing these manually (if you can import as-is, feel free to comment out the `sys.path.append`s)\n",
    "\n",
    "https://github.com/qubvel/segmentation_models.pytorch  \n",
    "https://timm.fast.ai/  \n",
    "https://github.com/lukemelas/EfficientNet-PyTorch\n",
    "\n",
    "The `pretrainedmodels` folder contains the weights we created during training.\n",
    "You might have to do a bit of manual massaging to get all the imports you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eb75315",
   "metadata": {
    "papermill": {
     "duration": 15.172294,
     "end_time": "2023-06-14T16:56:00.062639",
     "exception": false,
     "start_time": "2023-06-14T16:55:44.890345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('externals')\n",
    "\n",
    "\n",
    "# sys.path.append('/kaggle/input/pretrainedmodels/pretrainedmodels-0.7.4')\n",
    "# sys.path.append('/kaggle/input/efficientnet-pytorch/EfficientNet-PyTorch-master')\n",
    "# sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "# sys.path.append('/kaggle/input/segmentation-models-pytorch/segmentation_models.pytorch-master')\n",
    "# sys.path.append('/kaggle/input/unet3d/pytorch3dunet/pytorch3dunet')\n",
    "# sys.path.append('/kaggle/input/unet3d/pytorch3dunet')\n",
    "# sys.path.append('/kaggle/input/unet3d/')\n",
    "\n",
    "import segmentation_models_pytorch as smpx\n",
    "from pytorch3dunet.unet3d.model import get_model\n",
    "from unetr import UNETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eebbc05",
   "metadata": {
    "papermill": {
     "duration": 0.412184,
     "end_time": "2023-06-14T16:56:00.487302",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.075118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48345fd4",
   "metadata": {
    "papermill": {
     "duration": 0.0123,
     "end_time": "2023-06-14T16:56:00.512292",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.499992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a40f8bc1",
   "metadata": {
    "papermill": {
     "duration": 0.035624,
     "end_time": "2023-06-14T16:56:00.561034",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.525410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/core/validation.py:58: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "/tmp/ipykernel_167/595736528.py:71: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=[10, 50]),\n",
      "/tmp/ipykernel_167/595736528.py:76: UserWarning: Argument(s) 'max_holes, max_width, max_height, mask_fill_value' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=1, max_width=int(size * 0.3), max_height=int(size * 0.3),\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    comp_name = 'vesuvius'\n",
    "\n",
    "    # comp_dir_path = './'\n",
    "    comp_dir_path = '/workspace/input/'\n",
    "    comp_folder_name = 'vesuvius-challenge-ink-detection'\n",
    "    # comp_dataset_path = f'{comp_dir_path}datasets/{comp_folder_name}/'\n",
    "    comp_dataset_path = f'{comp_dir_path}{comp_folder_name}/'\n",
    "    \n",
    "    exp_name = '3d_unet_subv2'\n",
    "\n",
    "    # ============== pred target =============\n",
    "    target_size = 1\n",
    "\n",
    "    # ============== model cfg =============\n",
    "    model_name = '3d_unet_segformer'\n",
    "    backbone = 'None'\n",
    "#     backbone = 'se_resnext50_32x4d'\n",
    "\n",
    "    in_chans = 16\n",
    "    # ============== training cfg =============\n",
    "    size = 1024\n",
    "    tile_size = 1024\n",
    "    stride = tile_size // 1\n",
    "\n",
    "    batch_size = 3 # 32\n",
    "    use_amp = True\n",
    "\n",
    "    scheduler = 'GradualWarmupSchedulerV2'\n",
    "    # scheduler = 'CosineAnnealingLR'\n",
    "    epochs = 15\n",
    "\n",
    "    warmup_factor = 10\n",
    "    lr = 1e-4 / warmup_factor\n",
    "\n",
    "    # ============== fold =============\n",
    "    valid_id = 2\n",
    "\n",
    "    objective_cv = 'binary'  # 'binary', 'multiclass', 'regression'\n",
    "    metric_direction = 'maximize'  # maximize, 'minimize'\n",
    "    # metrics = 'dice_coef'\n",
    "\n",
    "    # ============== fixed =============\n",
    "    pretrained = True\n",
    "    inf_weight = 'best'  # 'best'\n",
    "\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-6\n",
    "    max_grad_norm = 1000\n",
    "\n",
    "    print_freq = 50\n",
    "    num_workers = 2\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(size, size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.75),\n",
    "        A.ShiftScaleRotate(p=0.75),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        A.CoarseDropout(max_holes=1, max_width=int(size * 0.3), max_height=int(size * 0.3), \n",
    "                        mask_fill_value=0, p=0.5),\n",
    "        # A.Cutout(max_h_size=int(size * 0.6),\n",
    "        #          max_w_size=int(size * 0.6), num_holes=1, p=1.0),\n",
    "        A.Normalize(\n",
    "            mean= [0] * in_chans,\n",
    "            std= [1] * in_chans\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(\n",
    "            mean= [0] * in_chans,\n",
    "            std= [1] * in_chans\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07166ff2",
   "metadata": {
    "papermill": {
     "duration": 0.023514,
     "end_time": "2023-06-14T16:56:00.597217",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.573703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IS_DEBUG = False\n",
    "mode = 'train' if IS_DEBUG else 'test'\n",
    "TH = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fc296dd",
   "metadata": {
    "papermill": {
     "duration": 0.090446,
     "end_time": "2023-06-14T16:56:00.700677",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.610231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171fcc0c",
   "metadata": {
    "papermill": {
     "duration": 0.012051,
     "end_time": "2023-06-14T16:56:00.725379",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.713328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d3181a",
   "metadata": {
    "papermill": {
     "duration": 0.024568,
     "end_time": "2023-06-14T16:56:00.762072",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.737504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n",
    "def rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    # pixels = (pixels >= thr).astype(int)\n",
    "    \n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba57cd",
   "metadata": {
    "papermill": {
     "duration": 0.012549,
     "end_time": "2023-06-14T16:56:00.787362",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.774813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9caa21a8",
   "metadata": {
    "papermill": {
     "duration": 0.027887,
     "end_time": "2023-06-14T16:56:00.827677",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.799790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_image(fragment_id):\n",
    "    images = []\n",
    "\n",
    "#     idxs = range(65)\n",
    "    mid = 65 // 2\n",
    "    start = mid - CFG.in_chans // 2\n",
    "    end = mid + CFG.in_chans // 2\n",
    "    idxs = range(start, end)\n",
    "\n",
    "    path = CFG.comp_dataset_path + f\"{mode}/{fragment_id}/surface_volume/\"\n",
    "    if os.path.exists(f\"{path}/00.jpg\"):\n",
    "        ext = \"jpg\"\n",
    "    else:\n",
    "        ext = \"tif\"\n",
    "\n",
    "    for i in tqdm(idxs):\n",
    "        \n",
    "        image = cv2.imread(f\"{path}{i:02}.{ext}\", 0)\n",
    "\n",
    "        pad0 = (CFG.tile_size - image.shape[0] % CFG.tile_size)\n",
    "        pad1 = (CFG.tile_size - image.shape[1] % CFG.tile_size)\n",
    "\n",
    "        image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "\n",
    "        images.append(image)\n",
    "    images = np.stack(images, axis=2)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f54b80e",
   "metadata": {
    "papermill": {
     "duration": 0.027028,
     "end_time": "2023-06-14T16:56:00.866823",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.839795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transforms(data, cfg):\n",
    "    if data == 'train':\n",
    "        aug = A.Compose(cfg.train_aug_list)\n",
    "    elif data == 'valid':\n",
    "        aug = A.Compose(cfg.valid_aug_list)\n",
    "\n",
    "    # print(aug)\n",
    "    return aug\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, cfg, labels=None, transform=None):\n",
    "        self.images = np.array(images)\n",
    "        self.cfg = cfg\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.xyxys)\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.load(self.images[idx])\n",
    "        data = self.transform(image=image)\n",
    "        image = data['image']\n",
    "        return image[None, :, :, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f476714",
   "metadata": {
    "papermill": {
     "duration": 0.027652,
     "end_time": "2023-06-14T16:56:00.906428",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.878776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_test_dataset(fragment_id):\n",
    "    test_images = read_image(fragment_id)\n",
    "    \n",
    "    x1_list = list(range(0, test_images.shape[1]-CFG.tile_size+1, CFG.stride))\n",
    "    y1_list = list(range(0, test_images.shape[0]-CFG.tile_size+1, CFG.stride))\n",
    "    \n",
    "    test_images_list = []\n",
    "    xyxys = []\n",
    "    for y1 in tqdm(y1_list):\n",
    "        for x1 in x1_list:\n",
    "            y2 = y1 + CFG.tile_size\n",
    "            x2 = x1 + CFG.tile_size\n",
    "            if test_images[y1:y2, x1:x2].max() != 0:\n",
    "                chunk = f\"chunks/{fragment_id}/{x1}_{y1}_{x2}_{y2}.npy\"\n",
    "                if not os.path.exists(chunk):\n",
    "                    np.save(chunk, test_images[y1:y2, x1:x2])\n",
    "                test_images_list.append(chunk)\n",
    "                xyxys.append((x1, y1, x2, y2))\n",
    "    del test_images\n",
    "    gc.collect()\n",
    "    xyxys = np.stack(xyxys)\n",
    "            \n",
    "    test_dataset = CustomDataset(test_images_list, CFG, transform=get_transforms(data='valid', cfg=CFG))\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=CFG.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    return test_loader, xyxys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a8db65",
   "metadata": {
    "papermill": {
     "duration": 0.011966,
     "end_time": "2023-06-14T16:56:00.930782",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.918816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26ff61-0019-4504-bbf0-3c270908e9c1",
   "metadata": {},
   "source": [
    "Below we define the configs for different models we used in the eventual ensemble. We changed which models we actually included depending on the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "043fad1b",
   "metadata": {
    "papermill": {
     "duration": 0.026857,
     "end_time": "2023-06-14T16:56:00.970244",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.943387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation, SegformerModel, SegformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9a31cc5",
   "metadata": {
    "papermill": {
     "duration": 0.027797,
     "end_time": "2023-06-14T16:56:01.009859",
     "exception": false,
     "start_time": "2023-06-14T16:56:00.982062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b1_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 256,\n",
    "  \"depths\": [\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40e6c1c5",
   "metadata": {
    "papermill": {
     "duration": 0.027332,
     "end_time": "2023-06-14T16:56:01.049571",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.022239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b2_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    6,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8372d5ee",
   "metadata": {
    "papermill": {
     "duration": 0.027397,
     "end_time": "2023-06-14T16:56:01.092770",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.065373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b4_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    8,\n",
    "    27,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "108de382",
   "metadata": {
    "papermill": {
     "duration": 0.027203,
     "end_time": "2023-06-14T16:56:01.133534",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.106331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_segformer_b5_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    6,\n",
    "    40,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b90b4da",
   "metadata": {
    "papermill": {
     "duration": 0.038961,
     "end_time": "2023-06-14T16:56:01.184541",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.145580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_3d_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    18,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"num_labels\":1,\n",
    "  \"num_channels\":32})\n",
    "cnn_3d_more_filters_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    18,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"num_labels\":1,\n",
    "  \"num_channels\":64})\n",
    "\n",
    "unet_3d_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    4,\n",
    "    18,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 3,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"num_labels\":1,\n",
    "  \"num_channels\":16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "561503be",
   "metadata": {
    "papermill": {
     "duration": 0.033891,
     "end_time": "2023-06-14T16:56:01.230741",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.196850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet_3d_jumbo_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    6,\n",
    "    40,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":1\n",
    "})\n",
    "\n",
    "unetr_multiclass_config = SegformerConfig(**{\n",
    "  \"architectures\": [\n",
    "    \"SegformerForImageClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.0,\n",
    "  \"classifier_dropout_prob\": 0.1,\n",
    "  \"decoder_hidden_size\": 768,\n",
    "  \"depths\": [\n",
    "    3,\n",
    "    6,\n",
    "    40,\n",
    "    3\n",
    "  ],\n",
    "  \"downsampling_rates\": [\n",
    "    1,\n",
    "    4,\n",
    "    8,\n",
    "    16\n",
    "  ],\n",
    "  \"drop_path_rate\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.0,\n",
    "  \"hidden_sizes\": [\n",
    "    64,\n",
    "    128,\n",
    "    320,\n",
    "    512\n",
    "  ],\n",
    "  \"image_size\": 224,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_eps\": 1e-06,\n",
    "  \"mlp_ratios\": [\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"model_type\": \"segformer\",\n",
    "  \"num_attention_heads\": [\n",
    "    1,\n",
    "    2,\n",
    "    5,\n",
    "    8\n",
    "  ],\n",
    "  \"num_channels\": 32,\n",
    "  \"num_encoder_blocks\": 4,\n",
    "  \"patch_sizes\": [\n",
    "    7,\n",
    "    3,\n",
    "    3,\n",
    "    3\n",
    "  ],\n",
    "  \"sr_ratios\": [\n",
    "    8,\n",
    "    4,\n",
    "    2,\n",
    "    1\n",
    "  ],\n",
    "  \"strides\": [\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    2\n",
    "  ],\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"num_labels\":3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95594b7c",
   "metadata": {
    "papermill": {
     "duration": 0.066905,
     "end_time": "2023-06-14T16:56:01.310516",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.243611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unetr import UNETR\n",
    "from externals.models import cnn3d_segformer, cnn3d_segformer_more_filters, unet3d_segformer, unet3d_segformer_jumbo, UNETR_Segformer, UNETR_SegformerMC\n",
    "# from externals.models import CNN3D_Segformer, Unet3D_Segformer, CNN3D_Unet, CNN3D_MANet, CNN3D_EfficientUnetplusplusb5, CNN3D_SegformerB4\n",
    "\n",
    "def build_model(cfg, model_arch = None):\n",
    "    print('model_name', cfg.model_name)\n",
    "    if model_arch == \"cnn3d\":\n",
    "        model = cnn3d_segformer(cfg)\n",
    "    if model_arch == \"cnn3d_more_filters\":\n",
    "        model = cnn3d_segformer_more_filters(cfg)\n",
    "    if model_arch == \"unet3d\":\n",
    "        model = unet3d_segformer(cfg)\n",
    "    if model_arch == \"unet3d_jumbo\":\n",
    "        model = unet3d_segformer_jumbo(cfg)\n",
    "    if model_arch == \"unetr\":\n",
    "        model = UNETR_Segformer(cfg)\n",
    "    if model_arch == \"unetr_mc\":\n",
    "        model = UNETR_SegformerMC(cfg)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f168c01f",
   "metadata": {
    "papermill": {
     "duration": 0.033329,
     "end_time": "2023-06-14T16:56:01.356342",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.323013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self, use_tta=False):\n",
    "        self.models = []\n",
    "        self.use_tta = use_tta\n",
    "    def tta_infer(self, model:nn.Module, x):\n",
    "        #x.shape=(batch,c,h,w)\n",
    "        shape=x.shape\n",
    "        x=[x,*[torch.rot90(x,k=i,dims=(-2,-1)) for i in range(1,4)]]\n",
    "        x=[model(single_x) for single_x in x]\n",
    "        x=torch.cat(x,dim=0)\n",
    "        x=x.reshape(4,shape[0],*shape[3:])\n",
    "        x=[torch.rot90(x[i],k=-i,dims=(-2,-1)) for i in range(4)]\n",
    "        x=torch.stack(x,dim=0)\n",
    "        return x.mean(0)\n",
    "                \n",
    "    def __call__(self, x):\n",
    "        if self.use_tta:\n",
    "            outputs = [self.tta_infer(model, x).to('cpu').numpy()\n",
    "                   for model in self.models]\n",
    "        else:\n",
    "            outputs = [model(x).mean(axis = 1).to('cpu').numpy()\n",
    "                       for model in self.models]\n",
    "        avg_preds = np.mean(outputs, axis=0)\n",
    "        return avg_preds\n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "def build_ensemble_model(model_path, model_arch):\n",
    "    model = EnsembleModel(use_tta = True)\n",
    "    _model = build_model(CFG, model_arch)\n",
    "    _model.to(device)\n",
    "    state = torch.load(model_path)\n",
    "    try:\n",
    "        _model.load_state_dict(state)\n",
    "    except:\n",
    "        _model = nn.DataParallel(_model)\n",
    "        _model.load_state_dict(state)\n",
    "    _model.eval()\n",
    "\n",
    "    model.add_model(_model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03fb4b-87c7-416e-b214-3234f64587eb",
   "metadata": {},
   "source": [
    "Make sure you have the folders with fragment IDs added here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e781a384",
   "metadata": {
    "papermill": {
     "duration": 0.041571,
     "end_time": "2023-06-14T16:56:01.410668",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.369097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if mode == 'test':\n",
    "    fragment_ids = sorted(os.listdir(CFG.comp_dataset_path + mode))\n",
    "else:\n",
    "    fragment_ids = [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89428df8-d638-4c9a-93c6-d7992c03c2b4",
   "metadata": {},
   "source": [
    "## Select models you want to use here\n",
    "\n",
    "We just commented/uncommented this list for models we trained when we did inference. You'll need pretrained weights / pretrained models here from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b7bf600f",
   "metadata": {
    "papermill": {
     "duration": 0.035023,
     "end_time": "2023-06-14T16:56:01.457896",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.422873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tuples = [\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_1024_3dcnn_segformer_best.pth\", \"segformer_config\": cnn_3d_config, \"score\": .75},\n",
    "#    {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#     \"weight_path\": \"/kaggle/input/3d-unet/3d_unet_segformer_1024_3d_unet_segformer_final_all_train.pth\", \"segformer_config\": unet_3d_config, \"score\":.78},\n",
    "#-    {\"model_arch\": \"unet3d\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 4,\n",
    "#    \"weight_path\": \"/workspace/input/3d-unet/3d_unet_segformer_512_3d_unet_segformer_final.pth\", \"segformer_config\": unet_3d_config, \"score\":.77},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_1024_full_train_3dcnn_segformer_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_1024_swa_slow_3dcnn_segformer_final_swa.pth\", \"segformer_config\": cnn_3d_config, \"score\".74},\n",
    "#     {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dunet_segformer_1024_swa_slow_3dunet_segformer_final_swa.pth\", \"segformer_config\": unet_3d_config, \"score\":.75},\n",
    "#    {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#     \"weight_path\": \"/kaggle/input/3d-unet/3dunet_segformer_1024_swa_slow_all_train_3dunet_segformer_final.pth\", \"segformer_config\": unet_3d_config, \"score\":.78},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_10_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_15_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_20_final.pth\", \"segformer_config\": cnn_3d_config, \"score\":.77},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_25_final.pth\", \"segformer_config\": cnn_3d_config},\n",
    "#    {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 3,\n",
    "#     \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_all_train_swa_3dcnn_segformer_final_swa.pth\", \"segformer_config\": cnn_3d_config, \"score\":.78},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 5,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b1_3dcnn_segformer_b1_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b1_config, \"score\":.71},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 4,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b2_3dcnn_segformer_b2_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b2_config, \"score\":.68},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_segformer_b4_3dcnn_segformer_b4_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.74},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_bigsegformer_3dcnn_bigsegformer_final.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\":.76},\n",
    "#    {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "#     \"weight_path\": \"/kaggle/input/3d-unet/b5_long_train_all_frags_3dcnn_segformer_b5_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\": .77},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b5_long_train_all_frags_3dcnn_segformer_b5_10_final.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\": .74},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b5_long_train_all_frags_3dcnn_segformer_b5_30_final.pth\", \"segformer_config\": cnn_3d_segformer_b5_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d_more_filters\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/b3_more_fmaps_3dcnn_segformerb364_final_swa.pth\", \"segformer_config\": cnn_3d_more_filters_config, \"score\": .74},\n",
    "    {\"model_arch\": \"cnn3d_more_filters\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "     \"weight_path\": \"/workspace/input/3d-unet/b3_more_fmaps_all_train_3dcnn_segformerb364_final_swa.pth\", \"segformer_config\": cnn_3d_more_filters_config, \"score\":.78},\n",
    "#     {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3d_unet_3dunet_b3_final_swa.pth\", \"segformer_config\": unet_3d_config, \"score\":.73},\n",
    "#     {\"model_arch\": \"unet3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3d_unet_all_train_3dunet_b3_final_swa.pth\", \"segformer_config\": unet_3d_config, \"score\":.76},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 4,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_512_b2_all_train_3dcnn_b2_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b2_config},\n",
    "    # ran at wrong resolution. Scored .73\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_768_b4_adam_3dcnn_b4_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.73},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 768, \"size\": 768, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_768_b4_adam_3dcnn_b4_final_swa.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.74},\n",
    "#     {\"model_arch\": \"cnn3d\", \"tile_size\": 768, \"size\": 768, \"batch_size\": 2,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/3dcnn_768_b4_adam_3dcnn_b4_final_swa_all_train.pth\", \"segformer_config\": cnn_3d_segformer_b4_config, \"score\":.75},\n",
    "#-    {\"model_arch\": \"unet3d_jumbo\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#     \"weight_path\": \"/workspace/input/3d-unet/Jumbo_Unet_Jumbo_Unet_69_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\":.79},\n",
    "#     {\"model_arch\": \"unet3d_jumbo\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/Jumbo_Unet_Jumbo_Unet_69_new_label_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\":.77},\n",
    "#     {\"model_arch\": \"unet3d_jumbo\", \"tile_size\": 1024, \"size\": 1024, \"batch_size\": 1,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/Jumbo_Unet_Jumbo_Unet_5_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\": .69},\n",
    "#     {\"model_arch\": \"unetr\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/jumbo_unetr_unetr_1245_final_swa_all_train.pth\", \"segformer_config\": unet_3d_jumbo_config},\n",
    "#     {\"model_arch\": \"unetr_mc\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "#      \"weight_path\": \"/kaggle/input/3d-unet/unetr_multiclass_512_b5_unet_final_4Ryan.pth\", \"segformer_config\": unetr_multiclass_config, \"score\":.77},\n",
    "    {\"model_arch\": \"unetr\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "     \"weight_path\": \"/workspace/input/3d-unet/jumbo_unetr_unetr_888_final_swa_all_train_long.pth\", \"segformer_config\": unet_3d_jumbo_config, \"score\":.82},\n",
    "#    {\"model_arch\": \"unetr_mc\", \"tile_size\": 512, \"size\": 512, \"batch_size\": 8,\n",
    "#     \"weight_path\": \"/kaggle/input/3d-unet/unetr_multiclass_NOVALIDATION_512_b5_unet_final_swa_all_train.pth\", \"segformer_config\": unetr_multiclass_config},\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8baedd7",
   "metadata": {
    "papermill": {
     "duration": 0.012363,
     "end_time": "2023-06-14T16:56:01.482977",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.470614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8128a58d",
   "metadata": {
    "papermill": {
     "duration": 0.02438,
     "end_time": "2023-06-14T16:56:01.520106",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.495726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_process(probability, threshold, min_size = 5000):\n",
    "    \"\"\"\n",
    "    Post processing of each predicted mask, components with lesser number of pixels\n",
    "    than `min_size` are ignored\n",
    "    \"\"\"\n",
    "    # don't remember where I saw it\n",
    "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    predictions = np.zeros_like(probability, np.float32)\n",
    "    num = 0\n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdd8a630",
   "metadata": {
    "papermill": {
     "duration": 0.022776,
     "end_time": "2023-06-14T16:56:01.555928",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.533152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec000a10",
   "metadata": {
    "papermill": {
     "duration": 0.041126,
     "end_time": "2023-06-14T16:56:01.609633",
     "exception": false,
     "start_time": "2023-06-14T16:56:01.568507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name 3d_unet_segformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd7be7db8894d5d80f40f1526c47584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475cbfef87bf4b0cb800bb481e260b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cnn3d_more_filters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be40d10502c437b9fb5b0c6e705ecf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_count_min: 1.0\n",
      "Showing image for cnn3d_more_filters\n",
      "scroll5-20241113090990_512_16_cnn3d_more_filters_prediction.png\n",
      "model_name 3d_unet_segformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960d7a6dd073464fad697aa91cea3336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7262eb7abb634ce5be939da3b9d8de0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running unet3d_jumbo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a69a71406d94c29bf80d28d21fd06d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_count_min: 1.0\n",
      "Showing image for unet3d_jumbo\n",
      "scroll5-20241113090990_512_16_unet3d_jumbo_prediction.png\n",
      "model_name 3d_unet_segformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b5 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b5 and are newly initialized because the shapes did not match:\n",
      "- segformer.encoder.patch_embeddings.0.proj.weight: found shape torch.Size([64, 3, 7, 7]) in the checkpoint and torch.Size([64, 32, 7, 7]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664d433caa91453abff01fe231b30de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9edba3e09c634707b53d10abe42f6765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running unetr\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaddac40a35745049edb77b28c4363c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_count_min: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167/3981958800.py:67: RuntimeWarning: invalid value encountered in divide\n",
      "  mask_pred = mask_pred/mask_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing image for unetr\n",
      "scroll5-20241113090990_256_16_unetr_prediction.png\n",
      "Creating ensemble prediction image\n",
      "Cleaning up\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'maks_pred_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m#inklabels_rle = rle(mask_pred)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m#results.append((fragment_id, inklabels_rle))\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleaning up\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m mask_pred, mask_count, maks_pred_image\n\u001b[1;32m    101\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m    102\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'maks_pred_image' is not defined"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "if True:\n",
    "    for fragment_id in fragment_ids:\n",
    "        mask_preds = None\n",
    "        last_res = None\n",
    "        for model_config in model_tuples:\n",
    "            mask_pred = None\n",
    "            mask_count = None\n",
    "            if last_res != model_config[\"size\"]:\n",
    "                for file in glob.glob(\"*.npy\"):\n",
    "                    os.remove(file)\n",
    "                last_res = model_config[\"size\"]\n",
    "            CFG.tile_size = model_config[\"tile_size\"]\n",
    "            CFG.size = model_config[\"size\"]\n",
    "            CFG.batch_size = model_config[\"batch_size\"]\n",
    "            CFG.stride = CFG.tile_size // 2\n",
    "            CFG.valid_aug_list = [\n",
    "                A.Resize(CFG.size, CFG.size),\n",
    "                A.Normalize(\n",
    "                    mean= [0] * CFG.in_chans,\n",
    "                    std= [1] * CFG.in_chans\n",
    "                ),\n",
    "                ToTensorV2(transpose_mask=True),\n",
    "            ]\n",
    "            CFG.segformer_config = model_config[\"segformer_config\"]\n",
    "            model_arch = model_config[\"model_arch\"]\n",
    "            model = build_ensemble_model(model_config[\"weight_path\"], model_arch)\n",
    "            test_loader, xyxys = make_test_dataset(fragment_id)\n",
    "\n",
    "            binary_mask = cv2.imread(CFG.comp_dataset_path + f\"{mode}/{fragment_id}/mask.png\", 0)\n",
    "            binary_mask = (binary_mask / 255).astype(int)\n",
    "\n",
    "            ori_h = binary_mask.shape[0]\n",
    "            ori_w = binary_mask.shape[1]\n",
    "            # mask = mask / 255\n",
    "\n",
    "            pad0 = (CFG.tile_size - binary_mask.shape[0] % CFG.tile_size)\n",
    "            pad1 = (CFG.tile_size - binary_mask.shape[1] % CFG.tile_size)\n",
    "\n",
    "            binary_mask = np.pad(binary_mask, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "            if mask_pred is None:\n",
    "                mask_pred = np.zeros(binary_mask.shape)\n",
    "                mask_count = np.zeros(binary_mask.shape)\n",
    "\n",
    "            print(f'Running {model_arch}')\n",
    "            for step, (images) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "                images = images.to(device)\n",
    "                batch_size = images.size(0)\n",
    "                with autocast():            \n",
    "                    with torch.no_grad():\n",
    "                        y_preds = model(images)\n",
    "\n",
    "                start_idx = step*CFG.batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                for i, (x1, y1, x2, y2) in enumerate(xyxys[start_idx:end_idx]):\n",
    "                    mask_pred[y1:y2, x1:x2] += y_preds[i]\n",
    "                    mask_count[y1:y2, x1:x2] += np.ones((CFG.tile_size, CFG.tile_size))\n",
    "            del test_loader\n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            mask_pred = mask_pred[:ori_h, :ori_w]\n",
    "            mask_count = mask_count[:ori_h, :ori_w]\n",
    "            binary_mask = binary_mask[:ori_h, :ori_w]\n",
    "\n",
    "            print(f'mask_count_min: {mask_count.min()}')\n",
    "            mask_pred = mask_pred/mask_count\n",
    "            mask_pred = torch.sigmoid(torch.tensor(mask_pred)).numpy()\n",
    "            if mask_preds is None:\n",
    "                mask_preds = mask_pred/len(model_tuples)\n",
    "            else:\n",
    "                mask_preds += mask_pred/len(model_tuples)\n",
    "\n",
    "            print(f'Showing image for {model_arch}')\n",
    "            #plt.imshow(mask_pred)\n",
    "            output_filename = f'{fragment_id}_{CFG.stride}_{CFG.in_chans}_{model_arch}_prediction.png'\n",
    "            print(output_filename)\n",
    "            mask_pred_image = np.nan_to_num(mask_pred.astype(float))  # Convert to float first to avoid overflow\n",
    "            mask_pred_image = ((mask_pred_image - mask_pred_image.min()) * 255 / (mask_pred_image.max() - mask_pred_image.min())).astype(np.uint8)\n",
    "            mask_pred_image=Image.fromarray(mask_pred_image)\n",
    "            mask_pred_image.save(output_filename)\n",
    "            model_pred_image = mask_pred_image\n",
    "            model_pred = mask_pred\n",
    "            #del mask_pred_image\n",
    "\n",
    "        mask_pred = (mask_preds >= TH).astype(int)\n",
    "        mask_pred *= binary_mask\n",
    "        #print('Postprocessing...')\n",
    "        #mask_pred = post_process(mask_pred.astype(float), TH, 10000).astype(int)\n",
    "        print('Creating ensemble prediction image')\n",
    "        #plt.imshow(mask_pred)\n",
    "        output_filename = f'{fragment_id}_{CFG.stride}_{CFG.in_chans}_ensemble_prediction.png'\n",
    "        mask_pred_image = mask_pred.astype(float)  # Convert to float first to avoid overflow\n",
    "        mask_pred_image = ((mask_pred_image - mask_pred_image.min()) * 255 / (mask_pred_image.max() - mask_pred_image.min())).astype(np.uint8)\n",
    "        mask_pred_image=Image.fromarray(mask_pred_image)\n",
    "        mask_pred_image.save(output_filename)\n",
    "        #inklabels_rle = rle(mask_pred)\n",
    "        #results.append((fragment_id, inklabels_rle))\n",
    "        print('Cleaning up')\n",
    "        del mask_pred, mask_count, mask_pred_image\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        #for file in glob.glob(\"*.npy\"):\n",
    "        #    os.remove(file)\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cc47a-0b59-44d1-bb64-73ff6bc3eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = np.nan_to_num(model_pred)\n",
    "print(mp)\n",
    "print(mp.min())\n",
    "print(mp.max())\n",
    "\n",
    "plt.imshow(mp)\n",
    "mask_pred_image = mp.astype(float)  # Convert to float first to avoid overflow\n",
    "mask_pred_image = ((mask_pred_image - mask_pred_image.min()) * 255 / (mask_pred_image.max() - mask_pred_image.min())).astype(np.uint8)\n",
    "mask_pred_image=Image.fromarray(mask_pred_image)\n",
    "mask_pred_image.save(output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 104.261062,
   "end_time": "2023-06-14T16:56:04.777148",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-14T16:54:20.516086",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
